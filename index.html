<html>
  <head>
    <meta charset="UTF-8">
    <style type="text/css">
      #title {text-align: center; font-family: arial;}
      #authors {text-align: center; font-style: italic; font-size: 19px; font-family: arial;}
      #abstract {text-align: center; font-family: arial;}
      #Audio-Samples {text-align: center; font-family: arial;}
      #Audio-Samples-detail {text-align: center; font-weight: normal; font-size: 17px; font-family: arial;}

      .container {width: 900px; margin: 30px auto;}
      .centered {position: relative;display: inline-block; padding: 1em;font-weight: normal;font-size: 17px;font-family: arial;}
      .sample {text-align: center; font-family: arial;}
      .container-sample1 {width: 1400px;margin: 30px auto;}
      .centered-sample1 {position: relative;display: inline-block;padding: 1em;font-weight: normal;font-size: 17px;font-family: arial;}
      .sample1 {font-style: italic; font-family: arial;}
      
    </style>
  </head>

  <body>
    <h1 id="title"><br />Few-Shot Speaker Adaptation Text-to-Speech using Style Equalizer</h1>
    <p id="authors">Yongmin Kim, Kyungseok Oh, Jeongki Min, Junyeop Lee, Bonhwa Ku, and Hanseok Ko</p>

    <h2 id="abstract"><br /><br />Abstract</h2>
    <div class="container">
      <div class="centered">
        Adaptive text-to-speech (ATTS) aims to adapt a source TTS model to synthesize a new target speaker's speech
        using few speech data based on finetuning. However, the adaptive text-to-speech based on finetuning typically
        requires several hours of high-quality speech per new target speakers and may also negatively impact the
        quality of speech synthesis for previously learned multi-speakers. In this paper, we propose an efficient few-shot
        adaptation TTS using a style equalizer module (SE-ATTS). In the proposed SE-ATTS, the style of
        new speaker to be synthesized is learned through the style characteristics of other speakers. This approach
        becomes key to mitigating the two issues in adaptive TTS mentioned above. Our experiments on datasets of
        Brand Engagement Network demonstrate the effectiveness of SE-ATTS method against existingmethods through
        subjective metrics. By integrating the style equalizer, SE-ATTS achieves remarkable speech synthesis 
        quality with limited speaker data, marking a significant step forward in addressing the challenges of 
        data-intensive TTS model training.
      </div>
    </div>

    <h2 id="Audio-Samples"><br /><br />Audio Samples</h2>
    <p id="Audio-Samples-detail"><br />All the audio sample use HiFi-GAN as a vocoder.</p>

    <h3 class="sample"><br /><br />Comparison According to Size of Adaptation Data</h3>

    <div class="container-sample1">
      <div class="centered-sample1">
        <p class="sample1">Text: The style guide has been amended accordingly.</p>
        <table>
          <thead>
            <tr>
              <th> </th>
              <th>Grond Truth</th>
              <th>Full Data</th>
              <th>20min</th>
              <th>10min</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>SE-ATTS(24spk)*</strong></td>
              <td><audio controls="controls">  <source type="audio/wav" src="data/GT/spk136_F_0103.wav" />&lt;/source&gt; </audio></td>
              <td><audio controls="controls">  <source type="audio/wav" src="data/few_sample/model_A/full/spk136_F_0103.wav" />&lt;/source&gt; </audio></td>
              <td><audio controls="controls">  <source type="audio/wav" src="data/few_sample/model_A/20min/spk136_F_0103.wav" />&lt;/source&gt; </audio></td>
              <td><audio controls="controls">  <source type="audio/wav" src="data/few_sample/model_A/10min/spk136_F_0103.wav" />&lt;/source&gt; </audio></td>
            </tr>
          </tbody>
        </table>

      </div>
    </div>
    
    
  </body>
</html>
