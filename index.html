<h1 style="text-align:center;font-family:arial;"><br />Few-Data Speaker Adaptation Text-to-Speech using Style Equalizer</h1>

<p style="text-align:center;font-style:italic;font-family:arial;">Yongmin Kim, Kyungseok Oh, Jeongki Min, Junyeop Lee, Bonhwa Ku, and Hanseok Ko</p>

<h2 style="text-align:center;font-family:arial;"><br /><br />ABSTRACT
<p style="margin-left:550px;font-weight:normal;font-size:15px;font-family:arial;"><br />Adaptive text-to-speech (ATTS) aims to adapt a source TTS model to synthesize a new target speaker's speech using few<br />
  speech data based on finetuning. However, the adaptive text-to-speech based on finetuning typically requires several hours of high-quality speech per new target speakers and may also negatively impact the quality of speech<br />
  synthesis for previously learned multi-speakers. In this paper, we propose an efficient few data adaptation TTS using a style equalizer module (SE-ATTS). In the proposed SE-ATTS, the style of new speaker to be synthesized<br />
  is learned through the style characteristics of other speakers. This approach becomes key to mitigating the two issues in adaptive TTS mentioned above. Our experiments on datasets of Brand Engagement Network Company<br />
  demonstrate the effectiveness of SE-ATTS method against existing methods through subjective metrics. By integrating the style equalizer, SE-ATTS achieves remarkable speech synthesis quality with limited speaker data, marking<br />
  a significant step forward in addressing the challenges of data-intensive TTS model training.
</p>
