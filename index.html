<html>
  <head>
    <meta charset="UTF-8">
    <style type="text/css">
      #title {text-align: center; font-family: arial;}
      #authors {text-align: center; font-style: italic; font-family: arial;}
      #abstract {text-align: center; font-family: arial;}
      #abstract-detail {text-align: left; margin-left: 550px; font-weight: normal; font-size: 17px; font-family: arial;}
    </style>
  </head>

  <body>
    <h1 id="title"><br />Few-Data Speaker Adaptation Text-to-Speech using Style Equalizer</h1>
    <p id="authors">Yongmin Kim, Kyungseok Oh, Jeongki Min, Junyeop Lee, Bonhwa Ku, and Hanseok Ko</p>

    <h2 id="abstract"><br /><br />ABSTRACT
    <p id="abstract-detail"><br />
      Adaptive text-to-speech (ATTS) aims to adapt a source TTS model to synthesize a new target speaker's speech using few<br />
      speech data based on finetuning. However, the adaptive text-to-speech based on finetuning typically requires several hours<br />
      of high-quality speech per new target speakers and may also negatively impact the quality of speech synthesis for previously<br />
      learned multi-speakers. In this paper, we propose an efficient few data adaptation TTS using a style equalizer module<br />
      (SE-ATTS). In the proposed SE-ATTS, the style of new speaker to be synthesized is learned through the style characteristics<br />
      of other speakers. This approach becomes key to mitigating the two issues in adaptive TTS mentioned above. Our experiments<br />
      on datasets of Brand Engagement Network Company demonstrate the effectiveness of SE-ATTS method against existing methods<br />
      through subjective metrics. By integrating the style equalizer, SE-ATTS achieves remarkable speech synthesis quality with<br />
      limited speaker data, marking a significant step forward in addressing the challenges of data-intensive TTS model training.<br />
    </p>
      
  </body>
</html>
